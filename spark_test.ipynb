{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "spark-test.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPAyMIVzx8/ftTldqQwxX89",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kkushal05/tutorials/blob/main/spark_test.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "\n",
        "sudo apt install -y mongodb >log\n",
        "service mongodb start"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s3ElhyPq3zJ7",
        "outputId": "68cf75bd-8047-4104-f9b2-2e7fc6389d0f"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " * Starting database mongodb\n",
            "   ...done.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "WARNING: apt does not have a stable CLI interface. Use with caution in scripts.\n",
            "\n",
            "debconf: unable to initialize frontend: Dialog\n",
            "debconf: (No usable dialog-like program is installed, so the dialog based frontend cannot be used. at /usr/share/perl5/Debconf/FrontEnd/Dialog.pm line 76, <> line 8.)\n",
            "debconf: falling back to frontend: Readline\n",
            "debconf: unable to initialize frontend: Readline\n",
            "debconf: (This frontend requires a controlling tty.)\n",
            "debconf: falling back to frontend: Teletype\n",
            "dpkg-preconfigure: unable to re-open stdin: \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "\n",
        "ps -ef | grep mongo"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "92eanyq83_92",
        "outputId": "1e5dd434-928f-4af0-80bb-44d3b58078e0"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "mongodb      705       1 83 09:51 ?        00:00:00 /usr/bin/mongod --config /etc/mongodb.conf\n",
            "root         737     735  0 09:51 ?        00:00:00 grep mongo\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pymongo import MongoClient\n",
        "client = MongoClient()\n",
        "client.list_database_names() "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1pOmvssI4FCE",
        "outputId": "5100b720-6b83-4ec6-9583-c15919dbed3a"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['admin', 'local']"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pandas\n",
        "!pip install pyspark\n",
        "!pip install pymongo\n",
        "!pip install sodapy\n",
        "!pip install progressbar"
      ],
      "metadata": {
        "id": "FTS218TP4kK0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bb8f98fb-aa6d-40c5-b8ba-da5ec44f15e8"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (1.3.5)\n",
            "Requirement already satisfied: numpy>=1.17.3 in /usr/local/lib/python3.7/dist-packages (from pandas) (1.21.6)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas) (2022.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas) (1.15.0)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: pyspark in /usr/local/lib/python3.7/dist-packages (3.3.0)\n",
            "Requirement already satisfied: py4j==0.10.9.5 in /usr/local/lib/python3.7/dist-packages (from pyspark) (0.10.9.5)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: pymongo in /usr/local/lib/python3.7/dist-packages (4.2.0)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: sodapy in /usr/local/lib/python3.7/dist-packages (2.1.1)\n",
            "Requirement already satisfied: requests>=2.20.0 in /usr/local/lib/python3.7/dist-packages (from sodapy) (2.23.0)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.20.0->sodapy) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.20.0->sodapy) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.20.0->sodapy) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.20.0->sodapy) (2022.6.15)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: progressbar in /usr/local/lib/python3.7/dist-packages (2.5)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sodapy import Socrata\n",
        "import pymongo\n",
        "from pymongo import MongoClient\n",
        "import pyspark\n",
        "from pyspark.sql import SparkSession\n",
        "import progressbar\n",
        "from pyspark.sql import Row\n",
        "import os\n",
        "\n",
        "client = Socrata(\"data.cityofnewyork.us\", None)\n",
        "start = 0\n",
        "chunk_size = 50\n",
        "results = []\n",
        "spark = SparkSession.builder.appName('sparkdf').getOrCreate()\n",
        "\n",
        "spark = SparkSession \\\n",
        "    .builder \\\n",
        "    .appName(\"mongodbtest1\") \\\n",
        "    .master('local')\\\n",
        "    .config(\"spark.mongodb.output.uri\", \"mongodb:/127.0.0.1:27017/raw_data\") \\\n",
        "    .config('spark.jars.packages', 'org.mongodb.spark:mongo-spark-connector_2.12:3.0.1') \\\n",
        "    .getOrCreate()\n",
        "\n",
        "mongo_client = MongoClient()\n",
        "db = mongo_client['raw_data']"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PMWIRiDWxLtz",
        "outputId": "7fcb8baf-9c52-4b07-a52e-5140f4411ceb"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:root:Requests made without an app_token will be subject to strict throttling limits.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def load_data_for_nypd_call_for_service():\n",
        "  start = 0\n",
        "  chunk_size = 50\n",
        "  data_id = \"n2zq-pubd\";\n",
        "  cols = \"NYPD_PCT_CD, BORO_NM, TYP_DESC, ARRIVD_TS, ADD_TS\"\n",
        "  db.call_for_service.drop()\n",
        "  record_count = client.get(data_id, where = \"ARRIVD_TS between '2022-02-01T00:00:00' and '2022-03-01T00:00:00'\" , select=\"COUNT(*)\")\n",
        "  print(record_count)\n",
        "  while True: \n",
        "    results = client.get(data_id, select=cols, where = \"ARRIVD_TS between '2022-02-01T00:00:00' and '2022-03-01T00:00:00'\", offset=start, limit=chunk_size)\n",
        "    dataframe = spark.createDataFrame(results)\n",
        "    dataframe = dataframe.na.drop()\n",
        "    print(\"Record count in  DB : \" + str(db.call_for_service.count_documents({})))\n",
        "    for item in dataframe.collect():\n",
        "      db.call_for_service.insert_one(item.asDict())\n",
        "    print(\"Record count in  DB : \" + str(db.call_for_service.count_documents({})))\n",
        "    start = start + chunk_size\n",
        "    if start > int(record_count[0]['COUNT']):\n",
        "        break\n",
        "\n",
        "#load_data_for_nypd_call_for_service()"
      ],
      "metadata": {
        "id": "BW99ahSn4bx8"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        " #----LOAD DATA FOR EMS BETWEEN A SPECIFIC DATE RANGE-------\n",
        "import requests\n",
        "import json\n",
        "\n",
        "def load_ems_data():\n",
        "  offset = 0\n",
        "  limit = 50\n",
        "  data_id = \"76xm-jjuj\";\n",
        "  cols = \"INCIDENT_DATETIME, BOROUGH, POLICEPRECINCT, FIRST_ON_SCENE_DATETIME\"\n",
        "  where_clause = \"$where=INCIDENT_DATETIME between '2022-01-01T00:00:00' and '2022-03-01T00:00:00'\";\n",
        "  response = requests.get(\"https://data.cityofnewyork.us/resource/76xm-jjuj.json?$select=COUNT(*)&\"+where_clause)\n",
        "  res_arr = json.loads(response.text)\n",
        "  record_count = res_arr[0]['COUNT']\n",
        "  db.ems.drop()\n",
        "  print(record_count)\n",
        "  while True:\n",
        "      url = \"https://data.cityofnewyork.us/resource/76xm-jjuj.json?\" + where_clause + \"&$limit=\"+str(limit)+\"&$offset=\"+str(offset);\n",
        "      results = requests.get(url)\n",
        "      dataframe = spark.createDataFrame(json.loads(results.text))\n",
        "      dataframe = dataframe.na.drop()\n",
        "      for item in dataframe.collect():\n",
        "        db.ems.insert_one(item.asDict())\n",
        "      print(\"Record count in  DB : \" + str(db.ems.count_documents({})))\n",
        "      offset = offset + limit\n",
        "      if offset > int(record_count):\n",
        "          break\n",
        "load_ems_data()"
      ],
      "metadata": {
        "id": "-F-9hSocsVjj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        " #----LOAD DATA FOR FIRE DEPT BETWEEN A SPECIFIC DATE RANGE-------\n",
        "def load_fire_dept_data():\n",
        "  start = 0\n",
        "  chunk_size = 50\n",
        "  data_id = \"8m42-w767\";\n",
        "  cols = \"INCIDENT_DATETIME, INCIDENT_BOROUGH, POLICEPRECINCT, FIRST_ON_SCENE_DATETIME, INCIDENT_CLASSIFICATION, INCIDENT_RESPONSE_SECONDS_QY\"\n",
        "  record_count = client.get(data_id, where = \"INCIDENT_DATETIME between '2021-02-01T00:00:00' and '2022-03-01T00:00:00'\", select=\"COUNT(*)\")\n",
        "  print(record_count)\n",
        "  db.fire_dept.drop()\n",
        "\n",
        "  while True: \n",
        "      results = client.get(data_id, select=cols, where=\"INCIDENT_DATETIME between '2021-02-01T00:00:00' and '2022-03-01T00:00:00'\", offset=start, limit=chunk_size)\n",
        "      dataframe = spark.createDataFrame(results)\n",
        "      dataframe = dataframe.na.drop()\n",
        "      for item in dataframe.collect():\n",
        "        db.fire_dept.insert_one(item.asDict())\n",
        "      print(\"Record count in  DB : \" + str(db.fire_dept.count_documents({})))\n",
        "      start = start + chunk_size\n",
        "      if start > int(record_count[0]['COUNT']):\n",
        "          break\n",
        "load_fire_dept_data()"
      ],
      "metadata": {
        "id": "YV3mTE12sXWw"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}